{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.opt_types import *\n",
    "from lib.utils import *\n",
    "from lib.part_one import f, f_full_data, x_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stochastic gradient methods - 30 Points\n",
    "\n",
    "\n",
    "In this problem, you will implement three different versions of stochastic gradient descent to solve the logistic regression problem on the entire NBA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the stochastic gradient descent methods, we recast our estimation problem [3](#mjx-eqn-eq3) as follows,\n",
    "\n",
    "\\begin{equation}\n",
    "\\tag{6}\n",
    "f(\\mathbf{x}) = \\frac{1}{n}\\sum_{i=1}^n\\bigg\\{\\underbrace{\\log(1 + \\exp(- b_i \\mathbf{a}_i^T\\mathbf{x})) + \\frac{\\mu}{2}\\|\\mathbf{x}\\|^2}_{f_i(\\mathbf{x})}\\bigg\\},\n",
    "\\end{equation}\n",
    "\n",
    "where we for notational convenience suppress the dependency on $\\mu$.\n",
    "As we saw in Problem 1, we have that\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla f_i(\\mathbf{x}) = -b_i \\sigma(-b_i \\cdot \\mathbf{a}_i^T\\mathbf{x})\\mathbf{a}_i + \\mu \\mathbf{x}.\n",
    "\\end{equation}\n",
    "\n",
    "The objective function can be written as a sum of $n$ terms. We augment the `Function` type introduced in previous notebooks to include the following attributes:\n",
    "\n",
    "- To access the gradient of the `j`-th term at a point `x` you can write `f.i_grad(j, x)`.\n",
    "- The number of terms $n$ is stored in the attribute `f.n`. \n",
    "\n",
    "Consider the following stochastic gradient update: At the iteration $k$, pick $i_k\\in\\{1, \\ldots, n\\}$ uniformly at random and define\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}^{k+1} :=  \\mathbf{x}^k - \\alpha_k\\nabla{f}_{i_k}(\\mathbf{x}^k).\\tag{SGD}\n",
    "\\end{equation}\n",
    "\n",
    "__(a)__ (2 points) Show that $\\nabla f_{i_k}(\\mathbf{x})$ is an unbiased estimation of $\\nabla f(\\mathbf{x})$. Explain why $\\nabla f_{i_k}$ is Lipschitz continuous with $L(f_{i_k}) = \\|\\mathbf{a}_{i_k}\\|^2+ \\mu $. \n",
    "\n",
    "__Hint__: Recall how we computed $L$ in Problem 1. In the following, we will set $L_{\\max} = \\max_{i\\in\\{1, \\ldots, n\\}}L(f_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to show that it is an unbiased estimator, we can see that the expected values of both is equal.\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathbb{E}[\\nabla f(x)] = \\mathbb{E}[\\frac{1}{n}\\sum_{i=1}^n\\nabla f_{i_k}(x)] =  \\frac{1}{n}\\sum_{i=1}^n\\mathbb{E}[\\nabla f_{i_k}(x)] = \\frac{1}{n}n\\mathbb{E}[\\nabla f_{i_k}(x)] = \\mathbb{E}[\\nabla f_{i_k}(x)] \n",
    "\n",
    "\n",
    "\\end{equation}\n",
    "\n",
    "So from the first part we showed that $\\nabla f_\\mu$ is L-Lipschitz with respect to the Euclidean norm, with \n",
    "\\begin{equation}\n",
    "\t\\mathbf{A} = \\begin{bmatrix}\n",
    "        \\leftarrow &  \\mathbf{a}_1^T & \\rightarrow \\\\\n",
    "        \\leftarrow &  \\mathbf{a}_2^T & \\rightarrow \\\\\n",
    "         &  \\ldots &  \\\\\n",
    "        \\leftarrow &  \\mathbf{a}_n^T & \\rightarrow \\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "\tL = \\|A\\|^2_F + \\mu \\text{, where }\\|\\cdot\\|_F\\text{ denotes the Frobenius norm. }\n",
    "\\end{equation}\n",
    "In our case we are looking at $f_{i_k}$, which is one of the components of $f_\\mu$. To be exact, $\\nabla f_\\mu$ is a sum of $\\nabla f_{i_k}$ . Thus, we can simply conclude that taking one of the components of $f_\\mu$ being $f_{i_k}$, it will amound to taking $a_{i}$ instead of the whole Hessian A. Therefore, the same inequalities that we showed in exercise 1 hold and we can find that the Lipschitz constant = $L(f_{i_k}) = \\|\\mathbf{a}_{i_k}\\|^2+ \\mu $. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__ (2 points) \n",
    "We can use the standard stochastic gradient descent method [SGD](#mjx-eqn-eqSGD) to solve [6](#mjx-eqn-eq6). \n",
    "Implement `SGD` by completing the following cells with $\\alpha_k =\\frac{0.01}{k}$.\n",
    "\n",
    "**Hint**: For some `N`, the function `np.random.choice(N)` generates a random integer in $\\{0, \\dots, N-1 \\}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SGD_state(OptState):\n",
    "    x_k: Vector\n",
    "    k: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_update(f, state):\n",
    "    x_k, k = state\n",
    "    \n",
    "    i_k = np.random.choice(f.n)\n",
    "    \n",
    "    next_x_k = x_k - (0.01/k*f.i_grad(i_k, x_k))\n",
    "    \n",
    "    return SGD_state(next_x_k, k+1)\n",
    "\n",
    "def SGD_initialize(f, x_zero):\n",
    "    return SGD_state(x_zero, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD = OptAlgorithm(name=\"SGD\", init_state=SGD_initialize, state_update=SGD_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(c)__ (6 points) \n",
    "Consider the following stochastic averaging gradient method [SAG](#mjx-eqn-eqSAG) to solve [6](#mjx-eqn-eq6):\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\text{pick } i_k\\in\\{1, \\ldots, n\\} \\text{ uniformly at random}\\\\\n",
    "\\mathbf{x}^{k+1} := \\mathbf{x}^k - \\frac{\\alpha_k}{n}\\sum_{i=1}^n\\mathbf{v}_i^k,\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "where \n",
    "\\begin{equation}\n",
    "\\mathbf{v}_i^k =\n",
    "\\begin{cases}\n",
    "\\nabla f_i(\\mathbf{x}^k) &\\text{if}\\, i = i_k,\\\\\n",
    "\\mathbf{v}_i^{k-1} &\\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Implement `SAG` by completing the following cells using the step-size \n",
    "$\\alpha_k=\\frac{0.01}{L_{\\max}}$ and $\\mathbf{v}^0=\\mathbf{0}$. (Note that you can access $L_{\\max}$ by writing `f.L_max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SAG_state(OptState):\n",
    "    x_k: Vector\n",
    "    v_k: List[Vector]\n",
    "    alpha_k: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAG_update(f, state):\n",
    "    x_k, v_k, alpha_k = state\n",
    "    i_k = np.random.choice(f.n)\n",
    "    v_k[i_k] = f.i_grad(i_k, x_k)\n",
    "    next_x_k = x_k - (alpha_k/f.n* sum(v_k))\n",
    "    \n",
    "    return SAG_state(next_x_k, v_k, alpha_k)\n",
    "\n",
    "def SAG_initialize(f, x_zero):\n",
    "    return SAG_state(x_zero, [0]*f.n, 0.01/f.L_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAG = OptAlgorithm(name=\"SAG\", init_state=SAG_initialize, state_update=SAG_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epochs([SGD, SAG], f, x_zero, 500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(d)__ (6 points) \n",
    "We can improve the convergence rate of SGD by periodically computing the full gradient. `SVRG` uses the following variance reduction scheme:\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\text{if } k = 0 \\text{ mod } q:\\\\\n",
    "\\quad \\mathbf{z} = \\mathbf{x}^k\\\\\n",
    "\\quad \\tilde{\\mathbf{v}} = \\nabla f(\\mathbf{x}^k)\\\\\n",
    "\\text{Pick } i_k\\in \\{1, \\ldots, n\\} \\text{ uniformly at random}\\\\\n",
    "\\mathbf{d}^k = \\nabla f_{i_k}({\\mathbf{x}}^k) - \\nabla f_{i_k}(\\mathbf{z}) + \\tilde{\\mathbf{v}}\\\\\n",
    "{\\mathbf{x}}^{k+1} := {\\mathbf{x}}^k - \\gamma\\mathbf{d}^k\\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Implement `SVRG` by completing the following cells and fixing the following constant:\n",
    "$\\gamma = 1/L_{\\max}$ and $q = 100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SVRG_state(OptState):\n",
    "    x_k: Vector \n",
    "    z: Vector\n",
    "    tilde_v: Vector\n",
    "    q: int\n",
    "    gamma: float\n",
    "    k: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVRG_update(f, state):\n",
    "    x_k, z, v_tilde, q, gamma, k = state\n",
    "    \n",
    "    if k % q == 0:\n",
    "        z = x_k\n",
    "        v_tilde = f.grad(x_k)\n",
    "    \n",
    "    i_k = np.random.choice(f.n)\n",
    "\n",
    "    d_k = f.i_grad(i_k, x_k) - f.i_grad(i_k, z) + v_tilde\n",
    "    next_x_k = x_k - (gamma*d_k)\n",
    "\n",
    "    return SVRG_state(next_x_k, z, v_tilde, q, gamma, k+1)\n",
    "    \n",
    "\n",
    "def SVRG_initialize(f, x_zero):\n",
    "    return SVRG_state(x_zero, x_zero, 0.0, 100, 1.0/f.L_max, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVRG = OptAlgorithm(name=\"SVRG\", init_state=SVRG_initialize, state_update=SVRG_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(e)__ (8 points) Another variance reduction method is `SARAH`. The scheme can be described as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\text{if } k = 0 \\text{ mod } q:\\\\\n",
    "\\quad {\\mathbf{x}}^k = {\\mathbf{z}}\\\\\n",
    "\\quad {\\mathbf{v}}^k = \\nabla f({\\mathbf{z}})\\\\\n",
    "\\quad t \\in \\{0, \\ldots, q-1\\} \\text{ uniformly at random}\\\\\n",
    "\\text{if } k = t \\text{ mod } q:\\\\\n",
    "\\quad {\\mathbf{z}} = {\\mathbf{x}}^{k}\\\\\n",
    "\\text{Pick } i_k\\in \\{1, \\ldots, n\\} \\text{ uniformly at random}\\\\\n",
    "{\\mathbf{v}}^{k+1} = \\nabla f_{i_k}({\\mathbf{x}}^k) - \\nabla f_{i_k}({\\mathbf{x}^{k-1}}) + {\\mathbf{v}}^{k}\\\\\n",
    "{\\mathbf{x}}^{k+1} := {\\mathbf{x}}^k - \\gamma {\\mathbf{v}}^{k+1}\\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Implement `SARAH` by completing the following cells. Pick `q=100` and $\\gamma = \\frac{1}{L_\\max}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SARAH_state(OptState):\n",
    "    x_k: Vector \n",
    "    prev_x_k: Vector\n",
    "    v_k: Vector\n",
    "    z: Vector\n",
    "    q: int\n",
    "    t: int\n",
    "    gamma: float\n",
    "    k: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARAH_update(f, state):\n",
    "\n",
    "    x_k, prev_x_k, v_k, z, q, t, gamma, k = state\n",
    "    \n",
    "    if k % q == 0:\n",
    "        x_k = z\n",
    "        v_k = f.grad(z)\n",
    "        t = np.random.choice(q)\n",
    "\n",
    "    if k % q == t:\n",
    "        z = x_k\n",
    "        \n",
    "    i_k = np.random.choice(f.n)\n",
    "\n",
    "    next_v_k =  f.i_grad(i_k,  x_k) - f.i_grad(i_k, prev_x_k) + v_k \n",
    "    next_x_k = x_k - (gamma * next_v_k)\n",
    "    \n",
    "    return SARAH_state(next_x_k, x_k, next_v_k, z, q, t, gamma, k+1)\n",
    "\n",
    "def SARAH_initialize(f, x_zero):\n",
    "     SARAH_state(x_zero, x_zero, 0.0, x_zero, 100, 0, 1.0/f.L_max, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARAH = OptAlgorithm(name=\"SARAH\", init_state=SARAH_initialize, state_update=SARAH_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(f)__ (6 points) Another variance reduction method is `Spider`. The scheme can be described as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\text{if } k = 0 \\text{ mod } q:\\\\\n",
    "\\quad {\\mathbf{v}}^k = \\nabla f({\\mathbf{x}}^k)\\\\\n",
    "\\text{if } k \\neq 0 \\text{ mod } q:\\\\\n",
    "\\quad \\text{Pick } i_k\\in \\{1, \\ldots, n\\} \\text{ uniformly at random}\\\\\n",
    "\\quad {\\mathbf{v}}^k = \\nabla f_{i_k}({\\mathbf{x}}^k) - \\nabla f_{i_k}({\\mathbf{x}}^{k-1}) + {\\mathbf{v}}^{k-1}\\\\\n",
    "\\eta := \\min \\big(\\frac{1}{\\left \\| {\\mathbf{v}}^{k} \\right \\|_2}, \\frac{1}{\\epsilon}\\big)\\\\\n",
    "{\\mathbf{x}}^{k+1} := {\\mathbf{x}}^k - \\gamma \\eta {\\mathbf{v}}^{k}\\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Implement `Spider` by completing the following cells. Pick `q=100`, $\\epsilon = 0.05$ and $\\gamma = \\frac{1}{L_\\max}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Spider_state(OptState):\n",
    "    x_k: Vector \n",
    "    prev_x_k: Vector\n",
    "    prev_v_k: Vector\n",
    "    q: int\n",
    "    gamma: float\n",
    "    k: int\n",
    "    epsilon: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def Spider_update(f, state):\n",
    "    x_k, prev_x_k, prev_v_k, q, gamma, k, epsilon = state\n",
    "    if k%q == 0:\n",
    "        v_k = f.grad(x_k)\n",
    "    else:\n",
    "        i_k = np.random.choice(f.n)\n",
    "        v_k = f.i_grad(i_k, x_k) - f.i_grad(i_k, prev_x_k) + prev_v_k\n",
    "    \n",
    "    mu = min(1.0/norm(prev_v_k), 1.0/epsilon)\n",
    "    next_x_k = x_k - (gamma*mu*v_k)\n",
    "    \n",
    "    return Spider_state(next_x_k, x_k, v_k, q, gamma, k+1, epsilon)\n",
    "\n",
    "def Spider_initialize(f, x_zero):\n",
    "    Spider_state(x_zero, x_zero, 0.0, 100, 1.0/f.L_max, 0, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spider = OptAlgorithm(name=\"Spider\", init_state=Spider_initialize, state_update=Spider_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epochs([SVRG, SARAH, Spider], f_full_data, x_zero, 50000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
